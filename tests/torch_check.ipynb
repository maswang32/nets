{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fbaee4e",
   "metadata": {},
   "source": [
    "# Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "671dee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from nets.functional import conv2d_forward, conv2d_backward, mse_loss_backward, mse_loss_forward\n",
    "\n",
    "from tests.check import check_equals\n",
    "\n",
    "N, C_in, C_out, H, W = 10, 4, 64, 123, 123\n",
    "stride= 3\n",
    "pad = 5\n",
    "K = 4\n",
    "num_groups= 2\n",
    "\n",
    "x = np.random.randn(N, C_in, H, W)\n",
    "kernel = np.random.randn(C_out, int(C_in/num_groups), K, K)\n",
    "b = np.random.randn(C_out)\n",
    "\n",
    "Hp = (H + 2*pad)\n",
    "Wp = (W + 2*pad)\n",
    "outH = int(np.ceil((Hp - (K - 1))/stride))\n",
    "outW = int(np.ceil((Wp - (K - 1)) / stride))\n",
    "\n",
    "y = np.random.randn(N, C_out, outH, outW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e78be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy Example\n",
    "y_hat, cache1 = conv2d_forward(x, kernel, b, pad=pad, stride=stride, num_groups=num_groups)\n",
    "loss, cache2 = mse_loss_forward(y_hat, y)\n",
    "\n",
    "dL_dyhat = mse_loss_backward(cache2)\n",
    "dL_dx, dL_dkernel, dL_db = conv2d_backward(dL_dyhat, cache1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "068b6a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7763568394002505e-14\n",
      "3.2526065174565133e-19\n",
      "1.8041124150158794e-16\n",
      "1.3877787807814457e-17\n"
     ]
    }
   ],
   "source": [
    "# Checking against Torch\n",
    "import torch\n",
    "x_torch = torch.tensor(x, requires_grad=True)\n",
    "kernel_torch = torch.tensor(kernel, requires_grad=True)\n",
    "b_torch = torch.tensor(b, requires_grad=True)\n",
    "y_torch = torch.tensor(y)\n",
    "\n",
    "yhat_torch = torch.nn.functional.conv2d(\n",
    "    x_torch, kernel_torch, b_torch, padding=pad, stride=stride, groups=num_groups\n",
    ")\n",
    "\n",
    "# Checking Forward Pass\n",
    "check_equals(y_hat, yhat_torch)\n",
    "loss = torch.nn.functional.mse_loss(yhat_torch, y_torch)\n",
    "loss.backward()\n",
    "\n",
    "check_equals(x_torch.grad, dL_dx)\n",
    "check_equals(kernel_torch.grad, dL_dkernel)\n",
    "check_equals(b_torch.grad, dL_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838870c",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7b37b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nets.functional import softmax_forward, softmax_backward\n",
    "N, K = 8, 10\n",
    "\n",
    "x = np.random.randn(N, K)\n",
    "y = np.random.randn(N, K)\n",
    "\n",
    "out, cache1 = softmax_forward(x)\n",
    "loss, cache2 = mse_loss_forward(out, y)\n",
    "\n",
    "dL_dout = mse_loss_backward(cache2)\n",
    "dL_dx = softmax_backward(dL_dout, cache1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd343b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.551115123125783e-17\n",
      "1.734723475976807e-18\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_torch = torch.tensor(x, requires_grad=True)\n",
    "y_torch = torch.tensor(y, requires_grad=True)\n",
    "\n",
    "out_torch = torch.nn.functional.softmax(x_torch, dim=-1)\n",
    "\n",
    "# Checking Forward Pass\n",
    "check_equals(out_torch, out)\n",
    "loss = torch.nn.functional.mse_loss(out_torch, y_torch)\n",
    "loss.backward()\n",
    "\n",
    "check_equals(x_torch.grad, dL_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deece894",
   "metadata": {},
   "source": [
    "# BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7baf3119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.881784197001252e-16\n",
      "9.367506770274758e-17\n",
      "5.551115123125783e-17\n",
      "5.551115123125783e-17\n"
     ]
    }
   ],
   "source": [
    "from nets.functional import batchnorm_forward, batchnorm_backward\n",
    "\n",
    "N, D = 8, 10\n",
    "\n",
    "x = np.random.randn(N, D)\n",
    "g = np.random.randn(D)\n",
    "b = np.random.randn(D)\n",
    "y = np.random.randn(N, D)\n",
    "\n",
    "y_hat, cache1 = batchnorm_forward(x, g, b)\n",
    "loss, cache2 = mse_loss_forward(y_hat, y)\n",
    "\n",
    "dL_dout = mse_loss_backward(cache2)\n",
    "dL_dx, dL_dg, dL_db = batchnorm_backward(dL_dout, cache1)\n",
    "\n",
    "\n",
    "# Checking against Torch\n",
    "x_torch = torch.tensor(x, requires_grad=True)\n",
    "g_torch = torch.tensor(g, requires_grad=True)\n",
    "b_torch = torch.tensor(b, requires_grad=True)\n",
    "y_torch = torch.tensor(y)\n",
    "\n",
    "yhat_torch = torch.nn.functional.batch_norm(\n",
    "    x_torch, None, None, g_torch, b_torch, training=True\n",
    ")\n",
    "\n",
    "# Checking Forward Pass\n",
    "check_equals(y_hat, yhat_torch)\n",
    "loss = torch.nn.functional.mse_loss(yhat_torch, y_torch)\n",
    "loss.backward()\n",
    "\n",
    "check_equals(x_torch.grad, dL_dx)\n",
    "check_equals(g_torch.grad, dL_dg)\n",
    "check_equals(b_torch.grad, dL_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7812b709",
   "metadata": {},
   "source": [
    "# LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0640f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.661338147750939e-16\n",
      "5.551115123125783e-17\n",
      "1.1102230246251565e-16\n",
      "1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "from nets.functional import layernorm_forward, layernorm_backward\n",
    "\n",
    "N, D = 8, 10\n",
    "\n",
    "x = np.random.randn(N, D)\n",
    "g = np.random.randn(D)\n",
    "b = np.random.randn(D)\n",
    "y = np.random.randn(N, D)\n",
    "\n",
    "y_hat, cache1 = layernorm_forward(x, g, b)\n",
    "loss, cache2 = mse_loss_forward(y_hat, y)\n",
    "\n",
    "dL_dout = mse_loss_backward(cache2)\n",
    "dL_dx, dL_dg, dL_db = layernorm_backward(dL_dout, cache1)\n",
    "\n",
    "\n",
    "# Checking against Torch\n",
    "x_torch = torch.tensor(x, requires_grad=True)\n",
    "g_torch = torch.tensor(g, requires_grad=True)\n",
    "b_torch = torch.tensor(b, requires_grad=True)\n",
    "y_torch = torch.tensor(y)\n",
    "\n",
    "yhat_torch = torch.nn.functional.layer_norm(\n",
    "    x_torch, (D,), g_torch, b_torch, eps=1e-5\n",
    ")\n",
    "\n",
    "# Checking Forward Pass\n",
    "check_equals(y_hat, yhat_torch)\n",
    "loss = torch.nn.functional.mse_loss(yhat_torch, y_torch)\n",
    "loss.backward()\n",
    "\n",
    "check_equals(x_torch.grad, dL_dx)\n",
    "check_equals(g_torch.grad, dL_dg)\n",
    "check_equals(b_torch.grad, dL_db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96dbe2f",
   "metadata": {},
   "source": [
    "# RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3329f67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.440892098500626e-16\n",
      "5.551115123125783e-17\n",
      "1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "from nets.functional import rms_norm_forward, rms_norm_backward\n",
    "\n",
    "N, D = 8, 10\n",
    "\n",
    "x = np.random.randn(N, D)\n",
    "g = np.random.randn(D)\n",
    "y = np.random.randn(N, D)\n",
    "\n",
    "y_hat, cache1 = rms_norm_forward(x, g)\n",
    "loss, cache2 = mse_loss_forward(y_hat, y)\n",
    "\n",
    "dL_dout = mse_loss_backward(cache2)\n",
    "dL_dx, dL_dg = rms_norm_backward(dL_dout, cache1)\n",
    "\n",
    "# Checking against Torch\n",
    "x_torch = torch.tensor(x, requires_grad=True)\n",
    "g_torch = torch.tensor(g, requires_grad=True)\n",
    "y_torch = torch.tensor(y)\n",
    "\n",
    "yhat_torch = torch.nn.functional.rms_norm(x_torch, (D,), g_torch, eps=1e-5)\n",
    "\n",
    "# Checking Forward Pass\n",
    "check_equals(y_hat, yhat_torch)\n",
    "loss = torch.nn.functional.mse_loss(yhat_torch, y_torch)\n",
    "loss.backward()\n",
    "\n",
    "check_equals(x_torch.grad, dL_dx)\n",
    "check_equals(g_torch.grad, dL_dg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf599ad",
   "metadata": {},
   "source": [
    "# BCE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99d2bb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1102230246251565e-16\n",
      "5.551115123125783e-17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from nets.functional import bce_loss_forward, bce_loss_backward\n",
    "from tests.check import check_equals\n",
    "\n",
    "N = 12\n",
    "p = np.random.rand(N)*0.5 + 0.25\n",
    "y = np.random.randint(2, size=(N,))\n",
    "\n",
    "\n",
    "\n",
    "loss, cache = bce_loss_forward(p, y)\n",
    "p_torch = torch.tensor(p, requires_grad=True)\n",
    "y_torch = torch.tensor(y, dtype=torch.float64)\n",
    "loss_torch = torch.nn.functional.binary_cross_entropy(p_torch, y_torch)\n",
    "\n",
    "\n",
    "dL_dp = bce_loss_backward(cache)\n",
    "check_equals(loss, loss_torch)\n",
    "loss_torch.backward()\n",
    "check_equals(p_torch.grad, dL_dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331666ea",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1af04b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 4, 100, 7)\n",
      "(12, 4, 100, 7)\n",
      "8.881784197001252e-16\n",
      "6.396614999689303e-05\n",
      "6.876525802156287e-05\n",
      "9.898514233929432e-05\n"
     ]
    }
   ],
   "source": [
    "from nets.functional import attn_forward, attn_backward, mse_loss_forward, mse_loss_backward \n",
    "B, H, S, T, D, C = 12, 4, 100, 200, 64, 7\n",
    "Q = np.random.randn(B, H, S, D)\n",
    "K = np.random.randn(B, H, T, D)\n",
    "V = np.random.randn(B, H, T, C)\n",
    "\n",
    "y = np.random.randn(B, H, S, C)\n",
    "\n",
    "\n",
    "out, cache1 = attn_forward(Q, K, V)\n",
    "print(out.shape)\n",
    "print(y.shape)\n",
    "\n",
    "loss, cache2 = mse_loss_forward(y, out)\n",
    "\n",
    "dL_dout = mse_loss_backward(cache2)\n",
    "dL_dQ, dL_dK, dL_dV = attn_backward(dL_dout, cache1)\n",
    "\n",
    "\n",
    "# Torch Check\n",
    "Q_torch = torch.tensor(Q, requires_grad=True)\n",
    "K_torch = torch.tensor(K, requires_grad=True)\n",
    "V_torch = torch.tensor(V, requires_grad=True)\n",
    "y_torch = torch.tensor(y)\n",
    "\n",
    "out_torch = torch.nn.functional.scaled_dot_product_attention(Q_torch, K_torch, V_torch)\n",
    "check_equals(out, out_torch)\n",
    "\n",
    "\n",
    "loss = torch.nn.functional.mse_loss(y_torch, out_torch)\n",
    "loss.backward()\n",
    "check_equals(Q_torch.grad, dL_dQ)\n",
    "check_equals(K_torch.grad, dL_dK)\n",
    "check_equals(V_torch.grad, dL_dV)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
